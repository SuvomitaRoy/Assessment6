{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a59166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suvom\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Hugging Face!\n",
      "\n",
      "I'm not sure if you've heard of the \"Hugging Face\" meme, but it's a meme that's been around for a while now. It's a meme that's been around for a\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "device = \"cpu\"  # Set to CPU explicitly\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "text = \"Hello, Hugging Face!\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_tokens = model.generate(**tokens, max_length=50)\n",
    "generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24776d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a distant land, the sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time in a distant land,\"\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "generated_tokens = model.generate(**tokens, max_length=50)\n",
    "generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05bd11f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a distant land, there was a man who had been sent by God to save the world. He was a man who had been sent by God to save the world. He was a man who had been sent by God to\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = model.generate(**tokens, \n",
    "                                  max_length=50, \n",
    "                                  do_sample=True, \n",
    "                                  top_k=50,  # Limits to top 50 probable words\n",
    "                                  top_p=0.9, # Adjusts probability threshold\n",
    "                                  num_beams=5) # Beam search for better sequences\n",
    "\n",
    "generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2679893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "def preprocess(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac01c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
